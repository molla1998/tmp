{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2116346a-bd7b-4539-b03d-51f02b4e8b85",
   "metadata": {},
   "source": [
    "# Generic Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ef9d4bc-571a-4eb7-9852-8892ccbf72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7da6412c-4b52-4bd7-bb4b-49dee339a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Info for noun: 'range' ---\n",
      "Matched Text: range\n",
      "Adjectives: ['mid']\n",
      "Adpositions: ['with']\n",
      "Is Part of Root: True\n",
      "\n",
      "--- Info for noun: 'price' ---\n",
      "Matched Text: price\n",
      "Adjectives: ['low']\n",
      "Adpositions: []\n",
      "Is Part of Root: False\n",
      "\n",
      "--- Modified Query (noun chunks and modifiers removed) ---\n",
      "show me phone\n"
     ]
    }
   ],
   "source": [
    "# User's input query\n",
    "sentence = \"show me mid range phone with low price\"\n",
    "\n",
    "# Dictionary or list of known nouns (lowercased)\n",
    "known_nouns = [\"range\", \"price\", \"end\"]\n",
    "\n",
    "doc = nlp(sentence)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "known_nouns_set = set(known_nouns)\n",
    "\n",
    "# Results and indices to remove\n",
    "results = []\n",
    "tokens_to_remove = set()\n",
    "\n",
    "#print(doc.noun_chunks)\n",
    "# Process noun chunks\n",
    "for chunk in doc.noun_chunks:\n",
    "    chunk_text = chunk.text.lower().strip()\n",
    "\n",
    "    for known in known_nouns_set:\n",
    "        if known in chunk_text:\n",
    "            related_adjs = []\n",
    "            related_adps = []\n",
    "            is_part_of_root_noun = False\n",
    "\n",
    "            # Check root relation\n",
    "            for token in chunk:\n",
    "                if token == root or token.head == root:\n",
    "                    is_part_of_root_noun = True\n",
    "\n",
    "            # Find ADJ/ADP from children of chunk tokens\n",
    "            for token in chunk:\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == \"ADJ\":\n",
    "                        related_adjs.append(child.text)\n",
    "                        tokens_to_remove.add(child.i)\n",
    "                    elif child.pos_ == \"ADP\":\n",
    "                        related_adps.append(child.text)\n",
    "                        tokens_to_remove.add(child.i)\n",
    "\n",
    "            # Mark entire noun chunk for removal\n",
    "            for token in chunk:\n",
    "               if str(token)==known:\n",
    "                    tokens_to_remove.add(token.i)\n",
    "\n",
    "            results.append({\n",
    "                \"matched_noun\": known,\n",
    "                \"matched_chunk\": chunk.text,\n",
    "                \"adjectives\": list(set(related_adjs)),\n",
    "                \"adpositions\": list(set(related_adps)),\n",
    "                \"is_part_of_root\": is_part_of_root_noun\n",
    "            })\n",
    "\n",
    "# Reconstruct sentence without removed tokens\n",
    "modified_query = \" \".join([\n",
    "    token.text for i, token in enumerate(doc) if i not in tokens_to_remove\n",
    "])\n",
    "\n",
    "# Output results\n",
    "for entry in results:\n",
    "    print(f\"\\n--- Info for noun: '{entry['matched_noun']}' ---\")\n",
    "    print(\"Matched Text:\", entry[\"matched_noun\"])\n",
    "    print(\"Adjectives:\", entry[\"adjectives\"])\n",
    "    print(\"Adpositions:\", entry[\"adpositions\"])\n",
    "    print(\"Is Part of Root:\", entry[\"is_part_of_root\"])\n",
    "\n",
    "print(\"\\n--- Modified Query (noun chunks and modifiers removed) ---\")\n",
    "print(modified_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42d51640-1d81-45d2-b4c7-8c56635b9ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1548643a-080d-4c14-b157-afa3c4e3b8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Info for noun: 'range' ---\n",
      "Matched Text: mid range phone\n",
      "Adjectives: []\n",
      "Adpositions: []\n",
      "Is Part of Root: False\n",
      "\n",
      "--- Info for noun: 'price' ---\n",
      "Matched Text: low price\n",
      "Adjectives: ['low']\n",
      "Adpositions: []\n",
      "Is Part of Root: False\n",
      "\n",
      "--- Modified Query (only matched nouns + modifiers removed) ---\n",
      "show me mid phone with\n"
     ]
    }
   ],
   "source": [
    "# User's input query\n",
    "sentence = \"show me mid range phone with low price\"\n",
    "\n",
    "# Dictionary or list of known nouns (lowercased)\n",
    "known_nouns = [\"range\", \"price\", \"end\"]\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "known_nouns_set = set(known_nouns)\n",
    "\n",
    "# Results and indices to remove\n",
    "results = []\n",
    "tokens_to_remove = set()\n",
    "\n",
    "# Process noun chunks\n",
    "for chunk in doc.noun_chunks:\n",
    "    for token in chunk:\n",
    "        # Only check noun or proper noun tokens\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"] and token.text.lower() in known_nouns_set:\n",
    "            related_adjs = []\n",
    "            related_adps = []\n",
    "            is_part_of_root_noun = token == root or token.head == root\n",
    "\n",
    "            # Check children for adjectives/adpositions\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"ADJ\":\n",
    "                    related_adjs.append(child.text)\n",
    "                    tokens_to_remove.add(child.i)\n",
    "                elif child.pos_ == \"ADP\":\n",
    "                    related_adps.append(child.text)\n",
    "                    tokens_to_remove.add(child.i)\n",
    "\n",
    "            # Mark the matched noun itself\n",
    "            tokens_to_remove.add(token.i)\n",
    "\n",
    "            results.append({\n",
    "                \"matched_noun\": token.text,\n",
    "                \"matched_chunk\": chunk.text,\n",
    "                \"adjectives\": list(set(related_adjs)),\n",
    "                \"adpositions\": list(set(related_adps)),\n",
    "                \"is_part_of_root\": is_part_of_root_noun\n",
    "            })\n",
    "\n",
    "# Reconstruct sentence without removed tokens\n",
    "modified_query = \" \".join([\n",
    "    token.text for i, token in enumerate(doc) if i not in tokens_to_remove\n",
    "])\n",
    "\n",
    "# Output results\n",
    "for entry in results:\n",
    "    print(f\"\\n--- Info for noun: '{entry['matched_noun']}' ---\")\n",
    "    print(\"Matched Text:\", entry[\"matched_chunk\"])\n",
    "    print(\"Adjectives:\", entry[\"adjectives\"])\n",
    "    print(\"Adpositions:\", entry[\"adpositions\"])\n",
    "    print(\"Is Part of Root:\", entry[\"is_part_of_root\"])\n",
    "\n",
    "print(\"\\n--- Modified Query (only matched nouns + modifiers removed) ---\")\n",
    "print(modified_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f4c2d-619f-4cc5-ad19-b14e17a16360",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dependency Tree Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f4f2bc-0934-4449-ad04-98eaf91b7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load medium English model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8026fc4-c001-4a14-9753-cb063aa3563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectives related to 'sunglasses': ['stylish']\n",
      "Adpositions related to 'sunglasses': ['for']\n",
      "Is 'sunglasses' part of the root noun or attached to the root? True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load medium English model\n",
    "#nlp = spacy.load(\"en_core_web_md\")\n",
    "# Input sentence\n",
    "sentence = \"find stylish sunglasses for men\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Define target noun phrase (case insensitive)\n",
    "target_noun = \"sunglasses\"\n",
    "\n",
    "# Variables to store results\n",
    "related_adjs = []\n",
    "related_adps = []\n",
    "is_part_of_root_noun = False\n",
    "\n",
    "# Identify root token\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "\n",
    "# Find noun chunks to check for the target noun\n",
    "for chunk in doc.noun_chunks:\n",
    "    if target_noun in chunk.text.lower():\n",
    "        \n",
    "        # Check if it's directly the root or attached to the root\n",
    "        for token in chunk:\n",
    "            if token == root or token.head == root:\n",
    "                is_part_of_root_noun = True\n",
    "        \n",
    "        # Find related ADJ and ADP via children\n",
    "        for token in chunk:\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"ADJ\":\n",
    "                    related_adjs.append(child.text)\n",
    "                if child.pos_ == \"ADP\":\n",
    "                    related_adps.append(child.text)\n",
    "\n",
    "# Remove duplicates\n",
    "related_adjs = list(set(related_adjs))\n",
    "related_adps = list(set(related_adps))\n",
    "\n",
    "# Print results\n",
    "print(f\"Adjectives related to '{target_noun}': {related_adjs}\")\n",
    "print(f\"Adpositions related to '{target_noun}': {related_adps}\")\n",
    "print(f\"Is '{target_noun}' part of the root noun or attached to the root? {is_part_of_root_noun}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97f9c9-b331-4b6d-8a51-917ece612190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load Ground Truth Data\n",
    "with open('test_queries.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# ðŸ”§ Dummy Infer Function (Replace with your actual extraction logic)\n",
    "def infer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Identify root token\n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    \n",
    "    results = []\n",
    "    seen_nouns = set()\n",
    "    id_counter = 1\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_text = chunk.text.lower()\n",
    "\n",
    "        # Avoid duplicates for multi-word noun chunks\n",
    "        if noun_text in seen_nouns:\n",
    "            continue\n",
    "        seen_nouns.add(noun_text)\n",
    "\n",
    "        related_adjs = []\n",
    "        related_adps = []\n",
    "        is_part_of_root_noun = False\n",
    "\n",
    "        # Check root attachment\n",
    "        for token in chunk:\n",
    "            if token == root or token.head == root:\n",
    "                is_part_of_root_noun = True\n",
    "\n",
    "        # Find related adjectives and adpositions\n",
    "        for token in chunk:\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"ADJ\":\n",
    "                    related_adjs.append(child.text)\n",
    "                if child.pos_ == \"ADP\":\n",
    "                    related_adps.append(child.text)\n",
    "\n",
    "        results.append({\n",
    "            \"id\": id_counter,\n",
    "            \"noun\": chunk.text.strip(),\n",
    "            \"adj\": related_adjs[0] if related_adjs else \"\",\n",
    "            \"adp\": related_adps[0] if related_adps else \"\",\n",
    "            \"part_of_root\": is_part_of_root_noun\n",
    "        })\n",
    "\n",
    "        id_counter += 1\n",
    "\n",
    "    return results\n",
    "    return [noun_entry for noun_entry in next(item['nouns'] for item in test_data if item['input'] == query)]\n",
    "\n",
    "# Evaluation Containers\n",
    "true_adj = []\n",
    "pred_adj = []\n",
    "\n",
    "true_adp = []\n",
    "pred_adp = []\n",
    "\n",
    "true_root = []\n",
    "pred_root = []\n",
    "\n",
    "# Evaluation Loop\n",
    "for item in test_data:\n",
    "    query = item['input']\n",
    "    gt_nouns = item['nouns']\n",
    "\n",
    "    # Run your model's extraction logic\n",
    "    predicted_nouns = infer(query)  # This should mirror the format: [{'noun': ..., 'adj': ..., 'adp': ..., 'part_of_root': ...}]\n",
    "\n",
    "    # Match by noun text\n",
    "    for gt in gt_nouns:\n",
    "        # Find matching noun in predictions\n",
    "        match = next((pred for pred in predicted_nouns if pred['noun'].lower() == gt['noun'].lower()), None)\n",
    "\n",
    "        if match:\n",
    "            true_adj.append(gt['adj'])\n",
    "            pred_adj.append(match['adj'])\n",
    "\n",
    "            true_adp.append(gt['adp'])\n",
    "            pred_adp.append(match['adp'])\n",
    "\n",
    "            true_root.append(gt['part_of_root'])\n",
    "            pred_root.append(match['part_of_root'])\n",
    "        else:\n",
    "            # Noun not found in prediction - count as miss\n",
    "            true_adj.append(gt['adj'])\n",
    "            pred_adj.append('')  # No match predicted\n",
    "\n",
    "            true_adp.append(gt['adp'])\n",
    "            pred_adp.append('')\n",
    "\n",
    "            true_root.append(gt['part_of_root'])\n",
    "            pred_root.append(False)\n",
    "\n",
    "# Helper to convert string matches to binary labels\n",
    "def to_binary(true_list, pred_list):\n",
    "    return [1 if t else 0 for t in true_list], [1 if p else 0 for p in pred_list]\n",
    "\n",
    "# ðŸŽ¯ Calculate Metrics for Adjectives\n",
    "adj_true_bin, adj_pred_bin = to_binary([bool(a) for a in true_adj], [bool(a) for a in pred_adj])\n",
    "print(\"Adjective Metrics:\")\n",
    "print(f\"Precision: {precision_score(adj_true_bin, adj_pred_bin):.2f}\")\n",
    "print(f\"Recall: {recall_score(adj_true_bin, adj_pred_bin):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(adj_true_bin, adj_pred_bin):.2f}\")\n",
    "print(f\"Accuracy: {accuracy_score(adj_true_bin, adj_pred_bin):.2f}\")\n",
    "print()\n",
    "\n",
    "# ðŸŽ¯ Calculate Metrics for Adpositions\n",
    "adp_true_bin, adp_pred_bin = to_binary([bool(a) for a in true_adp], [bool(a) for a in pred_adp])\n",
    "print(\"Adposition Metrics:\")\n",
    "print(f\"Precision: {precision_score(adp_true_bin, adp_pred_bin):.2f}\")\n",
    "print(f\"Recall: {recall_score(adp_true_bin, adp_pred_bin):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(adp_true_bin, adp_pred_bin):.2f}\")\n",
    "print(f\"Accuracy: {accuracy_score(adp_true_bin, adp_pred_bin):.2f}\")\n",
    "print()\n",
    "\n",
    "# ðŸŽ¯ Calculate Metrics for part_of_root\n",
    "root_true_bin, root_pred_bin = to_binary(true_root, pred_root)\n",
    "print(\"Part of Root Metrics:\")\n",
    "print(f\"Precision: {precision_score(root_true_bin, root_pred_bin):.2f}\")\n",
    "print(f\"Recall: {recall_score(root_true_bin, root_pred_bin):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(root_true_bin, root_pred_bin):.2f}\")\n",
    "print(f\"Accuracy: {accuracy_score(root_true_bin, root_pred_bin):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56087aca-beea-4851-a617-50b4b35a7010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective Metrics:\n",
      "Precision: 1.00\n",
      "Recall: 0.76\n",
      "F1 Score: 0.87\n",
      "Accuracy: 0.76\n",
      "\n",
      "Part of Root Metrics:\n",
      "Precision: 1.00\n",
      "Recall: 0.80\n",
      "F1 Score: 0.89\n",
      "Accuracy: 0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load spaCy model\n",
    "#nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Inference Function\n",
    "def infer(sentence, target_noun):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    \n",
    "    related_adjs = []\n",
    "    related_adps = []\n",
    "    is_part_of_root_noun = False\n",
    "\n",
    "    target_noun = target_noun.lower().strip()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        \n",
    "        if target_noun in chunk_text:\n",
    "            for token in chunk:\n",
    "                if token == root or token.head == root:\n",
    "                    is_part_of_root_noun = True\n",
    "\n",
    "            for token in chunk:\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == \"ADJ\":\n",
    "                        related_adjs.append(child.text.lower().strip())\n",
    "                    if child.pos_ == \"ADP\":\n",
    "                        related_adps.append(child.text.lower().strip())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    related_adjs = list(set(related_adjs))\n",
    "    related_adps = list(set(related_adps))\n",
    "\n",
    "    return {\n",
    "        \"adj\": related_adjs,\n",
    "        \"adp\": related_adps,\n",
    "        \"part_of_root\": is_part_of_root_noun\n",
    "    }\n",
    "\n",
    "# Load Ground Truth Data\n",
    "with open('test_json.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Evaluation Containers\n",
    "true_adj = []\n",
    "pred_adj = []\n",
    "\n",
    "true_adp = []\n",
    "pred_adp = []\n",
    "\n",
    "true_root = []\n",
    "pred_root = []\n",
    "\n",
    "# Evaluation Loop\n",
    "for item in test_data:\n",
    "    query = item['input']\n",
    "    gt_nouns = item['nouns']\n",
    "\n",
    "    for gt in gt_nouns:\n",
    "        target_noun = gt['noun']\n",
    "        \n",
    "        pred = infer(query, target_noun)\n",
    "        \n",
    "        gt_adjs = [a.lower().strip() for a in gt['adj']]\n",
    "        gt_adps = [a.lower().strip() for a in gt['adp']]\n",
    "\n",
    "        pred_adjs = [a.lower().strip() for a in pred['adj']]\n",
    "        pred_adps = [a.lower().strip() for a in pred['adp']]\n",
    "\n",
    "        true_adj.extend([1] * len(gt_adjs))\n",
    "        pred_adj.extend([1 if adj in pred_adjs else 0 for adj in gt_adjs])\n",
    "\n",
    "        true_adp.extend([1] * len(gt_adps))\n",
    "        pred_adp.extend([1 if adp in pred_adps else 0 for adp in gt_adps])\n",
    "\n",
    "        true_root.append(gt['part_of_root'])\n",
    "        pred_root.append(pred['part_of_root'])\n",
    "\n",
    "# Metric Helper\n",
    "def print_metrics(name, true_list, pred_list):\n",
    "    if len(true_list) == 0:\n",
    "        print(f\"{name} Metrics: No ground truth labels provided.\\n\")\n",
    "        return\n",
    "    print(f\"{name} Metrics:\")\n",
    "    print(f\"Precision: {precision_score(true_list, pred_list):.2f}\")\n",
    "    print(f\"Recall: {recall_score(true_list, pred_list):.2f}\")\n",
    "    print(f\"F1 Score: {f1_score(true_list, pred_list):.2f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(true_list, pred_list):.2f}\\n\")\n",
    "\n",
    "# ðŸŽ¯ Output Metrics\n",
    "print_metrics(\"Adjective\", true_adj, pred_adj)\n",
    "#print_metrics(\"Adposition\", true_adp, pred_adp)\n",
    "print_metrics(\"Part of Root\", [int(r) for r in true_root], [int(r) for r in pred_root])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85649c5-e8d4-432d-94ac-8b9791ed5be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Final Script -- Batch Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c27add5c-a519-41e1-8980-e1cb4ed99cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective Metrics:\n",
      "Precision: 1.00\n",
      "Recall: 0.76\n",
      "F1 Score: 0.87\n",
      "Accuracy: 0.76\n",
      "\n",
      "Adposition Metrics:\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "Accuracy: 0.00\n",
      "\n",
      "Part of Root Metrics:\n",
      "Precision: 1.00\n",
      "Recall: 0.80\n",
      "F1 Score: 0.89\n",
      "Accuracy: 0.91\n",
      "\n",
      "âœ… Reports generated: adj_report.csv, adp_report.csv, part_of_root_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\molla\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Inference Function\n",
    "def infer(sentence, target_noun):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    \n",
    "    related_adjs = []\n",
    "    related_adps = []\n",
    "    is_part_of_root_noun = False\n",
    "\n",
    "    target_noun = target_noun.lower().strip()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        \n",
    "        if target_noun in chunk_text:\n",
    "            for token in chunk:\n",
    "                if token == root or token.head == root:\n",
    "                    is_part_of_root_noun = True\n",
    "\n",
    "            for token in chunk:\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == \"ADJ\":\n",
    "                        related_adjs.append(child.text.lower().strip())\n",
    "                    if child.pos_ == \"ADP\":\n",
    "                        related_adps.append(child.text.lower().strip())\n",
    "    \n",
    "    related_adjs = list(set(related_adjs))\n",
    "    related_adps = list(set(related_adps))\n",
    "\n",
    "    return {\n",
    "        \"adj\": related_adjs,\n",
    "        \"adp\": related_adps,\n",
    "        \"part_of_root\": is_part_of_root_noun\n",
    "    }\n",
    "\n",
    "# Load Ground Truth Data\n",
    "with open('test_json.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Containers for Metrics and Reports\n",
    "true_adj = []\n",
    "pred_adj = []\n",
    "adj_report = []\n",
    "\n",
    "true_adp = []\n",
    "pred_adp = []\n",
    "adp_report = []\n",
    "\n",
    "true_root = []\n",
    "pred_root = []\n",
    "root_report = []\n",
    "\n",
    "# Evaluation Loop\n",
    "for item in test_data:\n",
    "    query = item['input']\n",
    "    gt_nouns = item['nouns']\n",
    "\n",
    "    for gt in gt_nouns:\n",
    "        target_noun = gt['noun']\n",
    "        \n",
    "        pred = infer(query, target_noun)\n",
    "        \n",
    "        gt_adjs = [a.lower().strip() for a in gt['adj']]\n",
    "        gt_adps = [a.lower().strip() for a in gt['adp']]\n",
    "\n",
    "        pred_adjs = [a.lower().strip() for a in pred['adj']]\n",
    "        pred_adps = [a.lower().strip() for a in pred['adp']]\n",
    "\n",
    "        # Adjective Evaluation & Report\n",
    "        for adj in gt_adjs:\n",
    "            true_adj.append(1)\n",
    "            pred_adj.append(1 if adj in pred_adjs else 0)\n",
    "            adj_report.append({\n",
    "                \"input_query\": query,\n",
    "                \"noun\": target_noun,\n",
    "                \"gt_adj\": adj,\n",
    "                \"pred_adj\": \", \".join(pred_adjs),\n",
    "                \"match\": adj in pred_adjs\n",
    "            })\n",
    "\n",
    "        # Adposition Evaluation & Report\n",
    "        for adp in gt_adps:\n",
    "            true_adp.append(1)\n",
    "            pred_adp.append(1 if adp in pred_adps else 0)\n",
    "            adp_report.append({\n",
    "                \"input_query\": query,\n",
    "                \"noun\": target_noun,\n",
    "                \"gt_adp\": adp,\n",
    "                \"pred_adp\": \", \".join(pred_adps),\n",
    "                \"match\": adp in pred_adps\n",
    "            })\n",
    "\n",
    "        # Part of Root Evaluation & Report\n",
    "        true_root.append(gt['part_of_root'])\n",
    "        pred_root.append(pred['part_of_root'])\n",
    "        root_report.append({\n",
    "            \"input_query\": query,\n",
    "            \"noun\": target_noun,\n",
    "            \"gt_part_of_root\": gt['part_of_root'],\n",
    "            \"pred_part_of_root\": pred['part_of_root'],\n",
    "            \"match\": gt['part_of_root'] == pred['part_of_root']\n",
    "        })\n",
    "\n",
    "# Metric Helper\n",
    "def print_metrics(name, true_list, pred_list):\n",
    "    if len(true_list) == 0:\n",
    "        print(f\"{name} Metrics: No ground truth labels provided.\\n\")\n",
    "        return\n",
    "    print(f\"{name} Metrics:\")\n",
    "    print(f\"Precision: {precision_score(true_list, pred_list):.2f}\")\n",
    "    print(f\"Recall: {recall_score(true_list, pred_list):.2f}\")\n",
    "    print(f\"F1 Score: {f1_score(true_list, pred_list):.2f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(true_list, pred_list):.2f}\\n\")\n",
    "\n",
    "# ðŸŽ¯ Output Metrics\n",
    "print_metrics(\"Adjective\", true_adj, pred_adj)\n",
    "print_metrics(\"Adposition\", true_adp, pred_adp)\n",
    "print_metrics(\"Part of Root\", [int(r) for r in true_root], [int(r) for r in pred_root])\n",
    "\n",
    "# Write CSV Reports\n",
    "with open(\"adj_report.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"input_query\", \"noun\", \"gt_adj\", \"pred_adj\", \"match\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(adj_report)\n",
    "\n",
    "with open(\"adp_report.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"input_query\", \"noun\", \"gt_adp\", \"pred_adp\", \"match\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(adp_report)\n",
    "\n",
    "with open(\"part_of_root_report.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"input_query\", \"noun\", \"gt_part_of_root\", \"pred_part_of_root\", \"match\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(root_report)\n",
    "\n",
    "print(\"âœ… Reports generated: adj_report.csv, adp_report.csv, part_of_root_report.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ebbba-9768-4a29-9650-34d73c45706f",
   "metadata": {},
   "source": [
    "# Standalone Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7ef67f-5e88-4fe4-90aa-d45601c1a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import time\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Inference Function\n",
    "def infer(sentence, target_noun):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    \n",
    "    related_adjs = []\n",
    "    related_adps = []\n",
    "    is_part_of_root_noun = False\n",
    "\n",
    "    target_noun = target_noun.lower().strip()\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        \n",
    "        if target_noun in chunk_text:\n",
    "            for token in chunk:\n",
    "                if token == root or token.head == root:\n",
    "                    is_part_of_root_noun = True\n",
    "\n",
    "            for token in chunk:\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == \"ADJ\":\n",
    "                        related_adjs.append(child.text.lower().strip())\n",
    "                    if child.pos_ == \"ADP\":\n",
    "                        related_adps.append(child.text.lower().strip())\n",
    "    \n",
    "    related_adjs = list(set(related_adjs))\n",
    "    related_adps = list(set(related_adps))\n",
    "\n",
    "    return {\n",
    "        \"adj\": related_adjs,\n",
    "        \"adp\": related_adps,\n",
    "        \"part_of_root\": is_part_of_root_noun\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf7b0a9-1b7b-46ec-898d-f3ef4387381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adj': ['affordable', 'cheap'], 'adp': ['with'], 'part_of_root': True}\n",
      "Inference Time: 8.258 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"show me cheap and affordable samsung s24 with compatible charger\"\n",
    "target_noun = \"samsung s24\"\n",
    "start_time = time.time()\n",
    "pred = infer(query, target_noun)\n",
    "end_time = time.time()\n",
    "print(pred)\n",
    "inference_time = (end_time - start_time) * 1000  # in milliseconds\n",
    "print(f\"Inference Time: {inference_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3944f-9e91-498e-92b8-efe6384c935b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bfc26fc-f097-4689-8dce-e7d4bab9b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Bulk Inference Function\n",
    "def infer_bulk(sentence, target_nouns):\n",
    "    doc = nlp(sentence)\n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Preprocess: lowercase and strip target nouns\n",
    "    target_nouns = [noun.lower().strip() for noun in target_nouns]\n",
    "\n",
    "    for target_noun in target_nouns:\n",
    "        related_adjs = []\n",
    "        related_adps = []\n",
    "        is_part_of_root_noun = False\n",
    "\n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_text = chunk.text.lower().strip()\n",
    "            if target_noun in chunk_text:\n",
    "                for token in chunk:\n",
    "                    if token == root or token.head == root:\n",
    "                        is_part_of_root_noun = True\n",
    "\n",
    "                for token in chunk:\n",
    "                    for child in token.children:\n",
    "                        if child.pos_ == \"ADJ\":\n",
    "                            related_adjs.append(child.text.lower().strip())\n",
    "                        if child.pos_ == \"ADP\":\n",
    "                            related_adps.append(child.text.lower().strip())\n",
    "\n",
    "        results[target_noun] = {\n",
    "            \"adj\": list(set(related_adjs)),\n",
    "            \"adp\": list(set(related_adps)),\n",
    "            \"part_of_root\": is_part_of_root_noun\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff851a3c-8b9d-44c4-b833-cedba0cc3e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s24 -> {'adj': [], 'adp': [], 'part_of_root': True}\n",
      "s25 -> {'adj': [], 'adp': [], 'part_of_root': True}\n",
      "s26 -> {'adj': [], 'adp': [], 'part_of_root': True}\n",
      "s27 -> {'adj': [], 'adp': [], 'part_of_root': False}\n",
      "\n",
      "Total Inference Time: 15.829 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"s24 ans s25 and s26 and s27\"\n",
    "target_nouns = [\"s24\", \"s25\", \"s26\", \"s27\"]\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = infer_bulk(query, target_nouns)\n",
    "end_time = time.time()\n",
    "\n",
    "for noun, result in predictions.items():\n",
    "    print(f\"{noun} -> {result}\")\n",
    "\n",
    "inference_time = (end_time - start_time) * 1000  # in milliseconds\n",
    "print(f\"\\nTotal Inference Time: {inference_time:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69416d-9525-48f1-92c4-dca68af60c8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ba53b31-75cd-4183-b01e-fe5b01b0004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def infer_bulk_dep(sentence, target_nouns):\n",
    "    doc = nlp(sentence)\n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    print(root)\n",
    "    # Normalize target nouns\n",
    "    target_nouns = [tn.lower().strip() for tn in target_nouns]\n",
    "    results = {tn: {\"adj\": [], \"adp\": [], \"part_of_root\": False} for tn in target_nouns}\n",
    "\n",
    "    # Build noun spans for all target nouns\n",
    "    target_token_map = {}\n",
    "    for noun in target_nouns:\n",
    "        for i in range(len(doc)):\n",
    "            for j in range(i+1, len(doc)+1):\n",
    "                span = doc[i:j]\n",
    "                if span.text.lower().strip() == noun:\n",
    "                    for token in span:\n",
    "                        target_token_map.setdefault(noun, set()).add(token)\n",
    "    \n",
    "    # Traverse tokens only once\n",
    "    for token in doc:\n",
    "        for noun, tokens in target_token_map.items():\n",
    "            if token in tokens:\n",
    "                # Check for adjective modifier\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"amod\":  # adjectival modifier\n",
    "                        results[noun][\"adj\"].append(child.text.lower())\n",
    "\n",
    "                # Check for prepositional modifier\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"prep\":\n",
    "                        results[noun][\"adp\"].append(child.text.lower())\n",
    "\n",
    "                # Check if this noun is part of the root phrase\n",
    "                if token == root or token.head == root:\n",
    "                    results[noun][\"part_of_root\"] = True\n",
    "\n",
    "    # Remove duplicates\n",
    "    for noun in results:\n",
    "        results[noun][\"adj\"] = list(set(results[noun][\"adj\"]))\n",
    "        results[noun][\"adp\"] = list(set(results[noun][\"adp\"]))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a718d2df-1322-48c2-91ce-131065ed21fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s24 -> {'adj': ['cheap'], 'adp': [], 'part_of_root': True}\n",
      "s25 -> {'adj': ['best'], 'adp': [], 'part_of_root': True}\n",
      "s26 -> {'adj': ['latest'], 'adp': [], 'part_of_root': False}\n",
      "s27 -> {'adj': [], 'adp': [], 'part_of_root': False}\n",
      "\n",
      "Total Inference Time: 11.883 ms\n"
     ]
    }
   ],
   "source": [
    "query = \"cheap s24 and best s25 and latest s26 and s27\"\n",
    "target_nouns = [\"s24\", \"s25\", \"s26\", \"s27\"]\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = infer_bulk_dep(query, target_nouns)\n",
    "end_time = time.time()\n",
    "\n",
    "for noun, result in predictions.items():\n",
    "    print(f\"{noun} -> {result}\")\n",
    "\n",
    "inference_time = (end_time - start_time) * 1000\n",
    "print(f\"\\nTotal Inference Time: {inference_time:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f57a36-e21f-4a9e-9ab7-caa6690442bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8a118e8-30e9-4b96-846e-6b535272635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def infer_optimized(sentence, target_nouns):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Normalize target nouns\n",
    "    target_nouns_norm = [noun.lower().strip() for noun in target_nouns]\n",
    "\n",
    "    # Setup PhraseMatcher to find target noun spans\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "    noun_patterns = [nlp.make_doc(noun) for noun in target_nouns_norm]\n",
    "    matcher.add(\"TARGET_NOUNS\", noun_patterns)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Map matched noun spans to their string keys\n",
    "    span_map = {}  # noun_text -> set of token indices\n",
    "    for match_id, start, end in matches:\n",
    "        span_text = doc[start:end].text.lower().strip()\n",
    "        span_map.setdefault(span_text, set()).update(range(start, end))\n",
    "\n",
    "    # Preprocess: Find root token once\n",
    "    root = next((token for token in doc if token.head == token), None)\n",
    "\n",
    "    # Prepare results\n",
    "    results = {noun: {\"adj\": [], \"adp\": [], \"part_of_root\": False} for noun in target_nouns_norm}\n",
    "\n",
    "    # One-pass token analysis\n",
    "    for token in doc:\n",
    "        for noun, indices in span_map.items():\n",
    "            if token.i in indices:\n",
    "                # Check if part of root phrase\n",
    "                if token == root or token.head == root:\n",
    "                    results[noun][\"part_of_root\"] = True\n",
    "\n",
    "                # Collect modifiers\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"amod\" and child.pos_ == \"ADJ\":\n",
    "                        results[noun][\"adj\"].append(child.text.lower())\n",
    "                    elif child.dep_ == \"prep\" and child.pos_ == \"ADP\":\n",
    "                        results[noun][\"adp\"].append(child.text.lower())\n",
    "\n",
    "    # Remove duplicates\n",
    "    for noun in results:\n",
    "        results[noun][\"adj\"] = list(set(results[noun][\"adj\"]))\n",
    "        results[noun][\"adp\"] = list(set(results[noun][\"adp\"]))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee062bd4-c397-41fc-a8f2-1fee2f10f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s24 -> {'adj': ['cheap'], 'adp': [], 'part_of_root': True}\n",
      "s25 -> {'adj': ['best'], 'adp': [], 'part_of_root': True}\n",
      "s26 -> {'adj': ['latest'], 'adp': [], 'part_of_root': False}\n",
      "s27 -> {'adj': [], 'adp': [], 'part_of_root': False}\n",
      "\n",
      "Total Inference Time: 7.514 ms\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"cheap s24 and best s25 and latest s26 and s27\"\n",
    "    target_nouns = [\"s24\", \"s25\", \"s26\", \"s27\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "    predictions = infer_optimized(query, target_nouns)\n",
    "    end_time = time.time()\n",
    "\n",
    "    for noun, result in predictions.items():\n",
    "        print(f\"{noun} -> {result}\")\n",
    "\n",
    "    inference_time = (end_time - start_time) * 1000\n",
    "    print(f\"\\nTotal Inference Time: {inference_time:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88f106-3085-426a-82ba-6e9bc4979552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "995b69aa-a5ea-431b-82fe-498e545df286",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Similarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f24ecf8-92f0-4cae-b13f-95bd9144f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading levenshtein-0.27.1-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Downloading levenshtein-0.27.1-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0014bf8a-5143-4610-a8f6-26fdbd477370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def char_level_similarity(gt_noun, ner_noun, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes character-level similarity using normalized Levenshtein distance.\n",
    "    \n",
    "    Args:\n",
    "        gt_noun (str): Ground truth string.\n",
    "        ner_noun (str): Predicted string.\n",
    "        threshold (float): Similarity threshold.\n",
    "        \n",
    "    Returns:\n",
    "        str: 'yes' if similarity >= threshold, else 'no'.\n",
    "    \"\"\"\n",
    "    gt = gt_noun.lower().strip()\n",
    "    ner = ner_noun.lower().strip()\n",
    "\n",
    "    if not gt or not ner:\n",
    "        return \"no\"\n",
    "\n",
    "    similarity = 1 - (Levenshtein.distance(gt, ner) / max(len(gt), len(ner)))\n",
    "\n",
    "    return \"yes\" if similarity >= threshold else \"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4888ac44-65e3-42c6-92c3-6dc054558a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "print(char_level_similarity(\"samsung s25 ultra\", \"s25 ultra\"))         # likely 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604a3e8-d750-4ebe-bf31-48ab071461b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Instatnt Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac9779e5-9997-406f-acb8-a1751e1a307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token      Dep        Head       POS        Children\n",
      "--------------------------------------------------\n",
      "case       ROOT       case       NOUN       ['for']\n",
      "for        prep       case       ADP        ['iphone']\n",
      "iphone     pobj       for        NOUN       []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cc51818b96ee48a297ee196a5ce37471-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">case</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">iphone</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cc51818b96ee48a297ee196a5ce37471-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cc51818b96ee48a297ee196a5ce37471-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M225.0,91.5 L233.0,79.5 217.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cc51818b96ee48a297ee196a5ce37471-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cc51818b96ee48a297ee196a5ce37471-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your e-commerce query\n",
    "query = \"case for iphone\"\n",
    "\n",
    "# Process the query\n",
    "doc = nlp(query)\n",
    "\n",
    "# Print dependency tree details\n",
    "print(f\"{'Token':<10} {'Dep':<10} {'Head':<10} {'POS':<10} {'Children'}\")\n",
    "print(\"-\" * 50)\n",
    "for token in doc:\n",
    "    children = [child.text for child in token.children]\n",
    "    print(f\"{token.text:<10} {token.dep_:<10} {token.head.text:<10} {token.pos_:<10} {children}\")\n",
    "\n",
    "# OPTIONAL: Visualize dependency tree (works in Jupyter or web contexts)\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2490777-b067-4e99-a4c5-b5c57dcff600",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# POS Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c23fe0-d7d0-47aa-b376-7280ba488bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary main noun: price\n",
      "Primary prev nouns: N/A\n",
      "Adj ref Primary noun: N/A\n",
      "ADP ref Primary noun: N/A\n",
      "Secondary main noun: sort\n",
      "Secondary prev nouns: phone\n",
      "Adj ref Secondary noun: N/A\n",
      "ADP ref Secondary noun: by\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define connectors with grammar-based rules\n",
    "left_primary = {\"with\", \"for\", \"under\"}\n",
    "right_primary = { \"of\", \"to\", \"from\", \"about\", \"on\", \"by\"}\n",
    "\n",
    "def extract_info(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize outputs\n",
    "    primary = {\"main_noun\": \"\", \"prev_nouns\": \"\", \"adj\": \"\", \"adp\": \"\"}\n",
    "    secondary = {\"main_noun\": \"\", \"prev_nouns\": \"\", \"adj\": \"\", \"adp\": \"\"}\n",
    "\n",
    "    connector_token = None\n",
    "\n",
    "    # Step 1: Find the first relevant connector\n",
    "    for token in doc:\n",
    "        if token.text.lower() in left_primary.union(right_primary) and token.pos_ == \"ADP\":\n",
    "            connector_token = token\n",
    "            break\n",
    "\n",
    "    if connector_token:\n",
    "        # Process left side\n",
    "        left_tokens = list(doc[:connector_token.i])\n",
    "        left_main_noun, left_prev_nouns = \"\", \"\"\n",
    "        for i in reversed(range(len(left_tokens))):\n",
    "            token = left_tokens[i]\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\",\"NUM\"}:\n",
    "                left_main_noun = token.text\n",
    "                prev_nouns = []\n",
    "                for j in reversed(range(i)):\n",
    "                    if left_tokens[j].pos_ in {\"NOUN\", \"PROPN\",\"NUM\"}:\n",
    "                        prev_nouns.insert(0, left_tokens[j].text)\n",
    "                    else:\n",
    "                        break\n",
    "                left_prev_nouns = \" \".join(prev_nouns)\n",
    "                break\n",
    "\n",
    "        # Process right side\n",
    "        right_tokens = list(doc[connector_token.i + 1:])\n",
    "        right_main_noun, right_prev_nouns = \"\", \"\"\n",
    "        for i, token in enumerate(right_tokens):\n",
    "            if token.pos_ in {\"NOUN\", \"PROPN\",\"NUM\"}:\n",
    "                right_main_noun = token.text\n",
    "                prev_nouns = []\n",
    "                for j in reversed(range(i)):\n",
    "                    if right_tokens[j].pos_ in {\"NOUN\", \"PROPN\",\"NUM\"}:\n",
    "                        prev_nouns.insert(0, right_tokens[j].text)\n",
    "                    else:\n",
    "                        break\n",
    "                right_prev_nouns = \" \".join(prev_nouns)\n",
    "                break\n",
    "\n",
    "        connector = connector_token.text.lower()\n",
    "\n",
    "        # Assign primary and secondary based on connector\n",
    "        if connector in left_primary:\n",
    "            primary[\"main_noun\"] = left_main_noun\n",
    "            primary[\"prev_nouns\"] = left_prev_nouns\n",
    "            secondary[\"main_noun\"] = right_main_noun\n",
    "            secondary[\"prev_nouns\"] = right_prev_nouns\n",
    "        elif connector in right_primary:\n",
    "            primary[\"main_noun\"] = right_main_noun\n",
    "            primary[\"prev_nouns\"] = right_prev_nouns\n",
    "            secondary[\"main_noun\"] = left_main_noun\n",
    "            secondary[\"prev_nouns\"] = left_prev_nouns\n",
    "\n",
    "        # Step 2: Find adjectives, adpositions, referring to nouns\n",
    "        for token in doc:\n",
    "            if token.head.text == primary[\"main_noun\"]:\n",
    "                if token.pos_ == \"ADJ\":\n",
    "                    primary[\"adj\"] = token.text\n",
    "                if token.pos_ == \"ADP\":\n",
    "                    primary[\"adp\"] = token.text\n",
    "            if token.head.text == secondary[\"main_noun\"]:\n",
    "                if token.pos_ == \"ADJ\":\n",
    "                    secondary[\"adj\"] = token.text\n",
    "                if token.pos_ == \"ADP\":\n",
    "                    secondary[\"adp\"] = token.text\n",
    "\n",
    "    return {\n",
    "        \"Primary main noun\": primary[\"main_noun\"],\n",
    "        \"Primary prev nouns\": primary[\"prev_nouns\"],\n",
    "        \"Adj ref Primary noun\": primary[\"adj\"],\n",
    "        \"ADP ref Primary noun\": primary[\"adp\"],\n",
    "        \"Secondary main noun\": secondary[\"main_noun\"],\n",
    "        \"Secondary prev nouns\": secondary[\"prev_nouns\"],\n",
    "        \"Adj ref Secondary noun\": secondary[\"adj\"],\n",
    "        \"ADP ref Secondary noun\": secondary[\"adp\"],\n",
    "    }\n",
    "\n",
    "# Test Example\n",
    "text = \"phone sort by price\"\n",
    "result = extract_info(text)\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value if value else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f39bad-7977-409a-b2b0-eee41c492d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary main noun: s24\n",
      "Primary prev nouns: -\n",
      "Adj ref Primary noun: -\n",
      "ADP ref Primary noun: with\n",
      "Secondary main noun: charger\n",
      "Secondary next nouns: -\n",
      "Adj ref Secondary noun: comapatible\n",
      "ADP ref Secondary noun: -\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define connectors with grammar-based rules\n",
    "left_primary = {\"with\", \"for\", \"under\",\"by\",\"on\"}\n",
    "right_primary = { }\n",
    "\n",
    "noun_pos_tags = {\"NOUN\", \"PROPN\", \"NUM\"}\n",
    "\n",
    "def extract_info(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    primary = {\"main_noun\": \"\", \"prev_nouns\": \"\", \"adj\": \"\", \"adp\": \"\"}\n",
    "    secondary = {\"main_noun\": \"\", \"next_nouns\": \"\", \"adj\": \"\", \"adp\": \"\"}\n",
    "\n",
    "    connector_token = None\n",
    "\n",
    "    # Find connector\n",
    "    for token in doc:\n",
    "        if token.text.lower() in left_primary.union(right_primary) and token.pos_ == \"ADP\":\n",
    "            connector_token = token\n",
    "            break\n",
    "\n",
    "    if connector_token:\n",
    "        left_tokens = list(doc[:connector_token.i])\n",
    "        right_tokens = list(doc[connector_token.i + 1:])\n",
    "\n",
    "        left_main_noun, left_prev_nouns = \"\", \"\"\n",
    "        right_main_noun, right_next_nouns = \"\", \"\"\n",
    "\n",
    "        # Find left main noun & previous consecutive nouns\n",
    "        for i in reversed(range(len(left_tokens))):\n",
    "            token = left_tokens[i]\n",
    "            if token.pos_ in noun_pos_tags:\n",
    "                left_main_noun = token.text\n",
    "                prev_nouns = []\n",
    "                for j in reversed(range(i)):\n",
    "                    if left_tokens[j].pos_ in noun_pos_tags:\n",
    "                        prev_nouns.insert(0, left_tokens[j].text)\n",
    "                    else:\n",
    "                        break\n",
    "                left_prev_nouns = \" \".join(prev_nouns)\n",
    "                break\n",
    "\n",
    "        # Find right main noun & next consecutive nouns\n",
    "        for i, token in enumerate(right_tokens):\n",
    "            if token.pos_ in noun_pos_tags:\n",
    "                right_main_noun = token.text\n",
    "                next_nouns = []\n",
    "                for j in range(i + 1, len(right_tokens)):\n",
    "                    if right_tokens[j].pos_ in noun_pos_tags:\n",
    "                        next_nouns.append(right_tokens[j].text)\n",
    "                    else:\n",
    "                        break\n",
    "                right_next_nouns = \" \".join(next_nouns)\n",
    "                break\n",
    "\n",
    "        connector = connector_token.text.lower()\n",
    "\n",
    "        if connector in left_primary:\n",
    "            primary[\"main_noun\"] = left_main_noun\n",
    "            primary[\"prev_nouns\"] = left_prev_nouns\n",
    "            secondary[\"main_noun\"] = right_main_noun\n",
    "            secondary[\"next_nouns\"] = right_next_nouns\n",
    "        elif connector in right_primary:\n",
    "            primary[\"main_noun\"] = right_main_noun\n",
    "            primary[\"prev_nouns\"] = right_next_nouns\n",
    "            secondary[\"main_noun\"] = left_main_noun\n",
    "            secondary[\"next_nouns\"] = left_prev_nouns\n",
    "\n",
    "        # Find adjectives, adpositions, verbs referring to nouns\n",
    "        for token in doc:\n",
    "            if token.head.text == primary[\"main_noun\"]:\n",
    "                if token.pos_ == \"ADJ\":\n",
    "                    primary[\"adj\"] = token.text\n",
    "                if token.pos_ == \"ADP\":\n",
    "                    primary[\"adp\"] = token.text\n",
    "\n",
    "            if token.head.text == secondary[\"main_noun\"]:\n",
    "                if token.pos_ == \"ADJ\":\n",
    "                    secondary[\"adj\"] = token.text\n",
    "                if token.pos_ == \"ADP\":\n",
    "                    secondary[\"adp\"] = token.text\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Primary main noun\": primary[\"main_noun\"],\n",
    "        \"Primary prev nouns\": primary[\"prev_nouns\"],\n",
    "        \"Adj ref Primary noun\": primary[\"adj\"],\n",
    "        \"ADP ref Primary noun\": primary[\"adp\"],\n",
    "        \"Secondary main noun\": secondary[\"main_noun\"],\n",
    "        \"Secondary next nouns\": secondary[\"next_nouns\"],\n",
    "        \"Adj ref Secondary noun\": secondary[\"adj\"],\n",
    "        \"ADP ref Secondary noun\": secondary[\"adp\"],\n",
    "    }\n",
    "\n",
    "# Test Example\n",
    "text = \"s24 with comapatible charger\"\n",
    "result = extract_info(text)\n",
    "\n",
    "# Pretty print\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value if value else '-'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e11798-9871-4669-95a3-ef7e232d93f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c8e8ff4-e470-46c4-ad7c-345224dc17df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           POS        Explanation\n",
      "----------------------------------------\n",
      "sort            ADV        adverb\n",
      "by              ADP        adposition\n",
      "price           NOUN       noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"sort by price\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print tokens and their POS tags\n",
    "print(f\"{'Token':15} {'POS':10} {'Explanation'}\")\n",
    "print(\"-\" * 40)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} {token.pos_:10} {spacy.explain(token.pos_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e6dcd-8acc-41cb-9006-7ffa86ecea4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a4a6b08-77d9-4e70-a248-054de46f9574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 ground truth examples saved to 'generated_ground_truth_dataset.json'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Word banks\n",
    "nouns = [\"phone\", \"laptop\", \"charger\", \"headphones\", \"tablet\", \"watch\", \"camera\", \"speaker\", \"mic\", \"router\", \"printer\", \"monitor\", \"keyboard\", \"mouse\", \"projector\", \"powerbank\", \"case\", \"screen\", \"earbuds\", \"gamepad\"]\n",
    "adjs = [\"cheap\", \"fast\", \"wireless\", \"affordable\", \"powerful\", \"compatible\", \"new\", \"slim\", \"durable\", \"portable\", \"lightweight\", \"stylish\", \"reliable\", \"advanced\", \"smart\"]\n",
    "nums = [\"10000\", \"500\", \"65w\", \"256gb\", \"2tb\", \"50\", \"100\", \"2000\", \"300\", \"150\"]\n",
    "adps = [\"with\", \"for\", \"under\", \"above\", \"beside\", \"without\"]\n",
    "\n",
    "# 50 sentence templates\n",
    "templates = [\n",
    "    \"Give me {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"I want {noun1} for {adj2} {noun2}.\",\n",
    "    \"Find {adj1} {noun1} under {num}.\",\n",
    "    \"Looking for {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Buy {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Get {noun1} for {adj2} {noun2}.\",\n",
    "    \"Search {adj1} {noun1} under {num}.\",\n",
    "    \"Order {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Cheap {noun1} for {adj2} {noun2}.\",\n",
    "    \"Looking for {adj1} {noun1} above {num}.\",\n",
    "    \"I need {adj1} {noun1} beside {adj2} {noun2}.\",\n",
    "    \"Get {noun1} without {adj2} {noun2}.\",\n",
    "    \"Find {adj1} {noun1} above {num}.\",\n",
    "    \"Order {adj1} {noun1} without {adj2} {noun2}.\",\n",
    "    \"New {noun1} with {adj2} {noun2}.\",\n",
    "    \"Affordable {noun1} for {adj2} {noun2}.\",\n",
    "    \"Looking for {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Buy {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"Search for {adj1} {noun1} under {num}.\",\n",
    "    \"Get {adj1} {noun1} above {num}.\",\n",
    "    \"Order {adj1} {noun1} beside {adj2} {noun2}.\",\n",
    "    \"Find {adj1} {noun1} without {adj2} {noun2}.\",\n",
    "    \"Need {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Looking for {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"Cheap {adj1} {noun1} under {num}.\",\n",
    "    \"Buy {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Order {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"I want {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Need {adj1} {noun1} under {num}.\",\n",
    "    \"Searching for {adj1} {noun1} above {num}.\",\n",
    "    \"Purchase {adj1} {noun1} without {adj2} {noun2}.\",\n",
    "    \"Get {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Find {adj1} {noun1} beside {adj2} {noun2}.\",\n",
    "    \"Affordable {adj1} {noun1} above {num}.\",\n",
    "    \"New {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"Order {adj1} {noun1} under {num}.\",\n",
    "    \"Need {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Searching {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"Get {adj1} {noun1} under {num}.\",\n",
    "    \"Looking for {adj1} {noun1} without {adj2} {noun2}.\",\n",
    "    \"Purchase {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"I want {adj1} {noun1} above {num}.\",\n",
    "    \"Find {adj1} {noun1} beside {adj2} {noun2}.\",\n",
    "    \"Buy {adj1} {noun1} without {adj2} {noun2}.\",\n",
    "    \"Affordable {adj1} {noun1} with {adj2} {noun2}.\",\n",
    "    \"Order {adj1} {noun1} for {adj2} {noun2}.\",\n",
    "    \"Looking for {adj1} {noun1} beside {adj2} {noun2}.\",\n",
    "    \"Purchase {adj1} {noun1} above {num}.\",\n",
    "    \"Need {adj1} {noun1} without {adj2} {noun2}.\",\n",
    "    \"Cheap {adj1} {noun1} for {adj2} {noun2}.\"\n",
    "]\n",
    "\n",
    "test_dataset = []\n",
    "\n",
    "for template in templates:\n",
    "    adj1 = random.choice(adjs)\n",
    "    adj2 = random.choice(adjs)\n",
    "    noun1 = random.choice(nouns)\n",
    "    noun2 = random.choice(nouns)\n",
    "    num = random.choice(nums)\n",
    "\n",
    "    sentence = template.format(adj1=adj1, adj2=adj2, noun1=noun1, noun2=noun2, num=num)\n",
    "\n",
    "    # Basic rule-based expected output logic\n",
    "    expected = {}\n",
    "\n",
    "    if \" with \" in template and \" for \" not in template:\n",
    "        expected = {\n",
    "            \"Primary main noun\": noun1,\n",
    "            \"Primary prev nouns\": \"\",\n",
    "            \"Adj ref Primary noun\": adj1,\n",
    "            \"ADP ref Primary noun\": \"\",\n",
    "            \"Verb ref Primary noun\": \"\",\n",
    "            \"Secondary main noun\": noun2,\n",
    "            \"Secondary next nouns\": \"\",\n",
    "            \"Adj ref Secondary noun\": adj2,\n",
    "            \"ADP ref Secondary noun\": \"\",\n",
    "            \"Verb ref Secondary noun\": \"\"\n",
    "        }\n",
    "    elif \" for \" in template:\n",
    "        expected = {\n",
    "            \"Primary main noun\": noun2,\n",
    "            \"Primary prev nouns\": \"\",\n",
    "            \"Adj ref Primary noun\": adj2,\n",
    "            \"ADP ref Primary noun\": \"\",\n",
    "            \"Verb ref Primary noun\": \"\",\n",
    "            \"Secondary main noun\": noun1,\n",
    "            \"Secondary next nouns\": \"\",\n",
    "            \"Adj ref Secondary noun\": adj1,\n",
    "            \"ADP ref Secondary noun\": \"\",\n",
    "            \"Verb ref Secondary noun\": \"\"\n",
    "        }\n",
    "    elif \" under \" in template or \" above \" in template:\n",
    "        adp = \"under\" if \" under \" in template else \"above\"\n",
    "        expected = {\n",
    "            \"Primary main noun\": noun1,\n",
    "            \"Primary prev nouns\": \"\",\n",
    "            \"Adj ref Primary noun\": adj1,\n",
    "            \"ADP ref Primary noun\": adp,\n",
    "            \"Verb ref Primary noun\": \"\",\n",
    "            \"Secondary main noun\": num,\n",
    "            \"Secondary next nouns\": \"\",\n",
    "            \"Adj ref Secondary noun\": \"\",\n",
    "            \"ADP ref Secondary noun\": \"\",\n",
    "            \"Verb ref Secondary noun\": \"\"\n",
    "        }\n",
    "    else:\n",
    "        expected = {\n",
    "            \"Primary main noun\": noun1,\n",
    "            \"Primary prev nouns\": \"\",\n",
    "            \"Adj ref Primary noun\": adj1,\n",
    "            \"ADP ref Primary noun\": \"\",\n",
    "            \"Verb ref Primary noun\": \"\",\n",
    "            \"Secondary main noun\": noun2,\n",
    "            \"Secondary next nouns\": \"\",\n",
    "            \"Adj ref Secondary noun\": adj2,\n",
    "            \"ADP ref Secondary noun\": \"\",\n",
    "            \"Verb ref Secondary noun\": \"\"\n",
    "        }\n",
    "\n",
    "    test_dataset.append({\n",
    "        \"text\": sentence,\n",
    "        \"expected\": expected\n",
    "    })\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"generated_ground_truth_dataset.json\", \"w\") as f:\n",
    "    json.dump(test_dataset, f, indent=4)\n",
    "\n",
    "print(\"Generated 50 ground truth examples saved to 'generated_ground_truth_dataset.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f9e20-e814-4924-91f5-2097481c7c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d9159-57af-4242-b9c4-f8987282ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d682d7-69b8-47a4-9b26-ad1415d72fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7eb522-07bb-4407-9e66-9e3f056eb4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55675844-9cfc-40e5-a507-9c2790f5d7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737aaafa-e3ea-4f5e-84da-22170c8c63a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d11161-ebcb-4122-9707-0c6d8c435a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define connectors with grammar-based rules\n",
    "left_primary = {\"with\", \"for\", \"under\",\"by\",\"on\"}\n",
    "right_primary = { }\n",
    "\n",
    "noun_pos_tags = {\"NOUN\", \"PROPN\", \"NUM\"}\n",
    "\n",
    "def extract_info(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    primary = {\"main_noun\": \"\", \"prev_nouns\": \"\", \"adj\": \"\", \"adp\": \"\"}\n",
    "    secondary = {\"main_noun\": \"\", \"next_nouns\": \"\", \"adj\": \"\", \"adp\": \"\"}\n",
    "\n",
    "    connector_token = None\n",
    "\n",
    "    # Find connector\n",
    "    for token in doc:\n",
    "        if token.text.lower() in left_primary.union(right_primary) and token.pos_ == \"ADP\":\n",
    "            connector_token = token\n",
    "            break\n",
    "\n",
    "    if connector_token:\n",
    "        left_tokens = list(doc[:connector_token.i])\n",
    "        right_tokens = list(doc[connector_token.i + 1:])\n",
    "\n",
    "        left_main_noun, left_prev_nouns = \"\", \"\"\n",
    "        right_main_noun, right_next_nouns = \"\", \"\"\n",
    "\n",
    "        # Find left main noun & previous consecutive nouns\n",
    "        for i in reversed(range(len(left_tokens))):\n",
    "            token = left_tokens[i]\n",
    "            if token.pos_ in noun_pos_tags:\n",
    "                left_main_noun = token.text\n",
    "                prev_nouns = []\n",
    "                for j in reversed(range(i)):\n",
    "                    if left_tokens[j].pos_ in noun_pos_tags:\n",
    "                        prev_nouns.insert(0, left_tokens[j].text)\n",
    "                    else:\n",
    "                        break\n",
    "                left_prev_nouns = \" \".join(prev_nouns)\n",
    "                break\n",
    "\n",
    "        # Find right main noun & next consecutive nouns\n",
    "        for i, token in enumerate(right_tokens):\n",
    "            if token.pos_ in noun_pos_tags:\n",
    "                right_main_noun = token.text\n",
    "                next_nouns = []\n",
    "                for j in range(i + 1, len(right_tokens)):\n",
    "                    if right_tokens[j].pos_ in noun_pos_tags:\n",
    "                        next_nouns.append(right_tokens[j].text)\n",
    "                    else:\n",
    "                        break\n",
    "                right_next_nouns = \" \".join(next_nouns)\n",
    "                break\n",
    "\n",
    "        connector = connector_token.text.lower()\n",
    "\n",
    "        if connector in left_primary:\n",
    "            primary[\"main_noun\"] = left_main_noun\n",
    "            primary[\"prev_nouns\"] = left_prev_nouns\n",
    "            secondary[\"main_noun\"] = right_main_noun\n",
    "            secondary[\"next_nouns\"] = right_next_nouns\n",
    "        elif connector in right_primary:\n",
    "            primary[\"main_noun\"] = right_main_noun\n",
    "            primary[\"prev_nouns\"] = right_next_nouns\n",
    "            secondary[\"main_noun\"] = left_main_noun\n",
    "            secondary[\"next_nouns\"] = left_prev_nouns\n",
    "\n",
    "        # Find adjectives, adpositions, verbs referring to nouns\n",
    "        for token in doc:\n",
    "            if token.head.text == primary[\"main_noun\"]:\n",
    "                if token.pos_ == \"ADJ\":\n",
    "                    primary[\"adj\"] = token.text\n",
    "                if token.pos_ == \"ADP\":\n",
    "                    primary[\"adp\"] = token.text\n",
    "\n",
    "            if token.head.text == secondary[\"main_noun\"]:\n",
    "                if token.pos_ == \"ADJ\":\n",
    "                    secondary[\"adj\"] = token.text\n",
    "                if token.pos_ == \"ADP\":\n",
    "                    secondary[\"adp\"] = token.text\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Primary main noun\": primary[\"main_noun\"],\n",
    "        \"Primary prev nouns\": primary[\"prev_nouns\"],\n",
    "        \"Adj ref Primary noun\": primary[\"adj\"],\n",
    "        \"ADP ref Primary noun\": primary[\"adp\"],\n",
    "        \"Secondary main noun\": secondary[\"main_noun\"],\n",
    "        \"Secondary next nouns\": secondary[\"next_nouns\"],\n",
    "        \"Adj ref Secondary noun\": secondary[\"adj\"],\n",
    "        \"ADP ref Secondary noun\": secondary[\"adp\"],\n",
    "    }\n",
    "\n",
    "# Test Example\n",
    "text = \"deals on galaxy s24\"\n",
    "result = extract_info(text)\n",
    "\n",
    "# Pretty print\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value if value else '-'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch, json

# âœ… lightweight open-source Korean instruction-tuned LLM
MODEL_NAME = "beomi/KoAlpaca-Polyglot-1.3B"

# ğŸ”„ Switch here: True = force GPU, False = CPU
USE_GPU = True

# -------------------------------
# Device selection
# -------------------------------
if USE_GPU and torch.cuda.is_available():
    device = "cuda"
    device_index = 0
else:
    device = "cpu"
    device_index = -1

print(f"ğŸš€ Loading {MODEL_NAME} on {device.upper()} ...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

if device == "cuda":
    model = model.to("cuda")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=device_index
)

# -------------------------------
# Function to extract modifiers
# -------------------------------
def extract_modifiers_llm(sentence: str):
    prompt = f"""
    ë¬¸ì¥ì—ì„œ ëª…ì‚¬ë¥¼ ì°¾ì•„ì„œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
    ê° ëª…ì‚¬ì— ëŒ€í•´:
    - ê´€ë ¨ í˜•ìš©ì‚¬(ìˆ˜ì‹ì–´)
    - ê´€ë ¨ ì¡°ì‚¬ (ì¡°ì‚¬/ê²©ì¡°ì‚¬)
    - ë£¨íŠ¸ ëª…ì‚¬ ì—¬ë¶€ (is_root: true/false)

    ë¬¸ì¥: "{sentence}"
    
    ì¶œë ¥ í˜•ì‹:
    [
      {{
        "noun": "...",
        "modifiers": ["..."],
        "adpositions": ["..."],
        "is_root": true/false
      }}
    ]
    """

    output = pipe(prompt, max_new_tokens=256, do_sample=False)[0]['generated_text']

    try:
        json_str = output[output.index("[") : output.rindex("]")+1]
        return json.loads(json_str)
    except Exception:
        return {"raw_output": output}


# -------------------------------
# âœ… Example Run
# -------------------------------
sentences = [
    "ì €ë ´í•œ ëƒ‰ì¥ê³  ë³´ì—¬ì¤˜",
    "ì‚¼ì„± S24í°ì„ ì‚¬ê³  ì‹¶ì–´",
    "í° ì§‘ì—ì„œ ì‚´ê³  ì‹¶ë‹¤"
]

for s in sentences:
    print(f"\nSentence: {s}")
    result = extract_modifiers_llm(s)
    print(json.dumps(result, ensure_ascii=False, indent=2))

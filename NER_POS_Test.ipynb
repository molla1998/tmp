{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e881a715-5628-4f51-89c9-da6d6ce5332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import time\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50b2a084-572b-47ef-9255-9959abae6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary or list of known nouns (lowercased)\n",
    "known_nouns = [\"range\", \"price\", \"battery capacity\", \"end\", \"weight\", \"capacity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a745159-5a68-4e39-b46c-95abebc4f494",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37ae9a1a-a9aa-49eb-9af1-01a487dbf5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 39.80 ms\n",
      "\n",
      "--- Info for matched noun: 'weight' ---\n",
      "Matched Text: weight\n",
      "Adjectives: ['light']\n",
      "Adpositions: []\n",
      "Is Part of Root: True\n",
      "\n",
      "--- Info for matched noun: 'price' ---\n",
      "Matched Text: price\n",
      "Adjectives: ['low']\n",
      "Adpositions: []\n",
      "Is Part of Root: False\n",
      "\n",
      "--- Modified Query (removed matched nouns and modifiers) ---\n",
      "cheap phone in\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Initialize PhraseMatcher with lowercased matching\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(text) for text in known_nouns]\n",
    "matcher.add(\"KNOWN_NOUN\", patterns)\n",
    "\n",
    "# Parse the sentence\n",
    "doc = nlp(sentence)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "\n",
    "# Match known phrases in the sentence\n",
    "matches = matcher(doc)\n",
    "\n",
    "# To avoid duplicate removal\n",
    "tokens_to_remove = set()\n",
    "results = []\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    matched_text = span.text\n",
    "    matched_noun = span.text.lower()\n",
    "\n",
    "    # Check if part of root noun\n",
    "    is_part_of_root_noun = any(token == root or token.head == root for token in span)\n",
    "\n",
    "    related_adjs = []\n",
    "    related_adps = []\n",
    "\n",
    "    for token in span:\n",
    "        for child in token.children:\n",
    "            if child.pos_ == \"ADJ\" or child.dep_ == \"amod\":\n",
    "                related_adjs.append(child.text)\n",
    "                tokens_to_remove.add(child.i)\n",
    "            if child.pos_ == \"ADP\":\n",
    "                related_adps.append(child.text)\n",
    "                tokens_to_remove.add(child.i)\n",
    "\n",
    "    # Mark span tokens for removal\n",
    "    for token in span:\n",
    "        tokens_to_remove.add(token.i)\n",
    "\n",
    "    results.append({\n",
    "        \"matched_noun\": matched_noun,\n",
    "        \"matched_text\": matched_text,\n",
    "        \"adjectives\": list(set(related_adjs)),\n",
    "        \"adpositions\": list(set(related_adps)),\n",
    "        \"is_part_of_root\": is_part_of_root_noun\n",
    "    })\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results\n",
    "print(f\"Inference time: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "# Build cleaned query\n",
    "modified_query = \" \".join(token.text for i, token in enumerate(doc) if i not in tokens_to_remove)\n",
    "\n",
    "# Output\n",
    "for entry in results:\n",
    "    print(f\"\\n--- Info for matched noun: '{entry['matched_noun']}' ---\")\n",
    "    print(\"Matched Text:\", entry[\"matched_text\"])\n",
    "    print(\"Adjectives:\", entry[\"adjectives\"])\n",
    "    print(\"Adpositions:\", entry[\"adpositions\"])\n",
    "    print(\"Is Part of Root:\", entry[\"is_part_of_root\"])\n",
    "\n",
    "print(\"\\n--- Modified Query (removed matched nouns and modifiers) ---\")\n",
    "print(modified_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f585e-7080-4d76-9f41-51f9918dda27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403bd733-6176-4efe-ab79-3d419293a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 13.09 ms\n",
      "\n",
      "--- Info for noun: 'weight' ---\n",
      "Matched Text: light weight cheap phone\n",
      "Adjectives: ['light', 'cheap']\n",
      "Adpositions: ['in']\n",
      "Is Part of Root: True\n",
      "\n",
      "--- Info for noun: 'price' ---\n",
      "Matched Text: low price\n",
      "Adjectives: ['low']\n",
      "Adpositions: []\n",
      "Is Part of Root: False\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "\n",
    "# Normalize known noun entries for fast lookup\n",
    "known_nouns_set = set(known_nouns)\n",
    "\n",
    "# Result storage\n",
    "results = []\n",
    "\n",
    "# Step 1: Extract all noun chunks from the query\n",
    "for chunk in doc.noun_chunks:\n",
    "    chunk_text = chunk.text.lower().strip()\n",
    "\n",
    "    # Check if full noun chunk or partial noun matches known nouns\n",
    "    for known in known_nouns_set:\n",
    "        if known in chunk_text:\n",
    "            related_adjs = []\n",
    "            related_adps = []\n",
    "            is_part_of_root_noun = False\n",
    "\n",
    "            # Check root attachment\n",
    "            for token in chunk:\n",
    "                if token == root or token.head == root:\n",
    "                    is_part_of_root_noun = True\n",
    "\n",
    "            # Check children for ADJ and ADP\n",
    "            for token in chunk:\n",
    "                for child in token.children:\n",
    "                    if child.pos_ == \"ADJ\":\n",
    "                        related_adjs.append(child.text)\n",
    "                        tokens_to_remove.add(child.i)\n",
    "                    if child.pos_ == \"ADP\":\n",
    "                        related_adps.append(child.text)\n",
    "                        tokens_to_remove.add(child.i)\n",
    "\n",
    "            results.append({\n",
    "                \"matched_noun\": known,\n",
    "                \"matched_chunk\": chunk.text,\n",
    "                \"adjectives\": list(set(related_adjs)),\n",
    "                \"adpositions\": list(set(related_adps)),\n",
    "                \"is_part_of_root\": is_part_of_root_noun\n",
    "            })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results\n",
    "print(f\"Inference time: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "# Output results\n",
    "for entry in results:\n",
    "    print(f\"\\n--- Info for noun: '{entry['matched_noun']}' ---\")\n",
    "    print(\"Matched Text:\", entry[\"matched_chunk\"])\n",
    "    print(\"Adjectives:\", entry[\"adjectives\"])\n",
    "    print(\"Adpositions:\", entry[\"adpositions\"])\n",
    "    print(\"Is Part of Root:\", entry[\"is_part_of_root\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b5414-cc4b-48c8-a537-63dec10f9b6b",
   "metadata": {},
   "source": [
    "# Standalone Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f4fc49b-42be-47b0-a041-a18f5abcf65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 8.72 ms\n",
      "\n",
      "--- Info for noun: 'weight' ---\n",
      "Matched Token: weight\n",
      "Adjectives: []\n",
      "Adpositions: []\n",
      "Is Part of Root: False\n",
      "\n",
      "--- Modified Query (nouns and modifiers removed) ---\n",
      "light phone\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sentence = \"light weight phone\"\n",
    "known_nouns_set = set(known_nouns)\n",
    "\n",
    "doc = nlp(sentence)\n",
    "root = [token for token in doc if token.head == token][0]\n",
    "\n",
    "# Results and indices to remove\n",
    "results = []\n",
    "tokens_to_remove = set()\n",
    "\n",
    "# Process each token (not chunks)\n",
    "for token in doc:\n",
    "    if token.text.lower() in known_nouns_set:\n",
    "        known = token.text.lower()\n",
    "        related_adjs = []\n",
    "        related_adps = []\n",
    "        #is_part_of_root_noun = (token == root or token.head == root)\n",
    "\n",
    "        # Check for adjective and adposition children\n",
    "        for child in token.children:\n",
    "            if child.pos_ == \"ADJ\" or child.dep_ == \"amod\" :\n",
    "                related_adjs.append(child.text)\n",
    "                tokens_to_remove.add(child.i)\n",
    "            if child.pos_ == \"ADP\":\n",
    "                related_adps.append(child.text)\n",
    "                tokens_to_remove.add(child.i)\n",
    "\n",
    "        # Mark noun itself for removal\n",
    "        tokens_to_remove.add(token.i)\n",
    "\n",
    "        results.append({\n",
    "            \"matched_noun\": known,\n",
    "            \"matched_token\": token.text,\n",
    "            \"adjectives\": list(set(related_adjs)),\n",
    "            \"adpositions\": list(set(related_adps)),\n",
    "            \"is_part_of_root\": is_part_of_root_noun\n",
    "        })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print results\n",
    "print(f\"Inference time: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "# Reconstruct modified query (excluding removed tokens)\n",
    "modified_query = \" \".join(\n",
    "    token.text for i, token in enumerate(doc) if i not in tokens_to_remove\n",
    ")\n",
    "\n",
    "# Output results\n",
    "for entry in results:\n",
    "    print(f\"\\n--- Info for noun: '{entry['matched_noun']}' ---\")\n",
    "    print(\"Matched Token:\", entry[\"matched_token\"])\n",
    "    print(\"Adjectives:\", entry[\"adjectives\"])\n",
    "    print(\"Adpositions:\", entry[\"adpositions\"])\n",
    "    print(\"Is Part of Root:\", entry[\"is_part_of_root\"])\n",
    "\n",
    "print(\"\\n--- Modified Query (nouns and modifiers removed) ---\")\n",
    "print(modified_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e06a85-b08e-4f2f-ac24-c6d3fe4b00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
